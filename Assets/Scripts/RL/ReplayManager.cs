//------------------------------------------------------------------------------
// <auto-generated>
//     이 코드는 도구를 사용하여 생성되었습니다.
//     런타임 버전:4.0.30319.34014
//
//     파일 내용을 변경하면 잘못된 동작이 발생할 수 있으며, 코드를 다시 생성하면
//     이러한 변경 내용이 손실됩니다.
// </auto-generated>
//------------------------------------------------------------------------------
using System;
using System.Collections.Generic;
using System.Linq;
using ANN;
namespace Learning
{
		public class ReplayManager
		{
			StateManager sm;
			NeuralNetwork[] nnArray;
			List<Experience> expList;
			bool backward;
		
				public ReplayManager ( ref NeuralNetwork[] _networks, ref StateManager _sm )
				{
					nnArray = _networks;
					sm = _sm;
					expList = new List<Experience>();
					backward = false;
				}
				
				public void addExp( State before, int action, State after, double reward, bool special ){
//					if( before == null )
//						;
//					if( after == null )
//						;
					expList.Add( new Experience( before.id, action, after.id, reward, special ) );
				}
				
				public void reset(){
					expList.Clear ();
					backward = false;
				}
				
				public void replay( double discountFactor ){
					if( expList.Count == 0 )
						return ;
					if( !backward ){
						expList.Reverse();
						backward = true;
					}
					
					double[] inputs = new double[nnArray[0].InputLayer.Count];
					inputs[inputs.Length - 1] = 1;
					double[] nextQValues = new double[nnArray.Length];
					double[] targetQ = new double[1];
					State before, after;
					bool special = false;
					foreach( Experience e in expList ){
						before = sm.getState( e.beforeStateId );
						
						if( e.special ){
							targetQ[0] = e.reward;
						}
						else{
							after = sm.getState( e.afterStateId );
							after.sensors.CopyTo( inputs, 0 );
							for( int i = 0; i < nextQValues.Length; i++ )
								nextQValues[i] = nnArray[i].Compute( inputs )[0];
							targetQ[0] = e.reward + discountFactor * nextQValues.Max();
						}
						before.sensors.CopyTo( inputs, 0 );
						nnArray[e.action].Train( inputs );
						nnArray[e.action].BackPropagate( targetQ );
					}
				
				}
		}
}

